paillier_pubkey<<A>> = fate_script.init_public_key()
pub_key<<H>> = paillier_pubkey<<A>>
pub_key<<D>> = paillier_pubkey<<A>>
pub_key<<G>> = paillier_pubkey<<A>>
pub_key<<E>> = paillier_pubkey<<A>>

X<<H>> = fate_script.get_lr_x_table("/data/projects/fate/python/examples/data/breast_a.csv")
W<<H>>= fate_script.get_lr_w("/data/projects/fate/python/examples/data/breast_a.csv")
X<<G>> = fate_script.get_lr_x_table("/data/projects/fate/python/examples/data/breast_b.csv")
W<<G>>= fate_script.get_lr_w("/data/projects/fate/python/examples/data/breast_b.csv")
Y<<G>> = fate_script.get_lr_y_table("/data/projects/fate/python/examples/data/breast_b.csv")
shape_w<<G>> = fate_script.get_lr_shape_w("/data/projects/fate/python/examples/data/breast_b.csv")


ml_conf<<A>> = fate_script.init_ml_conf()
ml_conf<<H>> = ml_conf<<A>>
ml_conf<<D>> = ml_conf<<A>>
ml_conf<<G>> = ml_conf<<A>>
ml_conf<<E>> = ml_conf<<A>>
ml_conf<<R>> = ml_conf<<A>>

is_stopped<<A>> = False
pre_loss<<A>> = None

for iter_idx in range(ml_conf.iter_num):
    for batch_idx in range(ml_conf.batch_num):
        
        forward<<H>> = X<<H>> @ W<<H>>
        [[forward]]<<H>> = forward<<H>>
        [[forward_square]]<<H>> = forward<<H>>**2

        [[forward_h]]<<G>> = [[forward]]<<H>>
        [[forward_square_h]]<<G>> = [[forward_square]]<<H>>

        forward<<G>> = X<<G>> @ W<<G>>
        [[forward]]<<G>> = forward<<G>>
        [[forward_square]]<<G>> = forward<<G>>**2

        [[agg_wx]]<<G>> = [[forward]]<<G>> + [[forward_h]]<<G>>

        [[agg_wx_square]]<<G>> = [[forward_square]]<<G>> + [[forward_square_h]]<<G>> + 2 * forward<<G>> * [[forward_h]]<<G>>

        [[fore_gradient]]<<G>> = 0.25 * [[agg_wx]]<<G>> - 0.5 * Y<<G>>

        [[grad_G]]<<G>> = (X<<G>> * [[fore_gradient]]<<G>>).mean()
        [[grad_H]]<<H>> = (X<<H>> * [[fore_gradient]]<<G>>).mean()

        [[grad_g]]<<A>> = [[grad_G]]<<G>>
        [[grad_h]]<<A>> = [[grad_H]]<<H>>

        grad<<A>> = [[grad_g]]<<A>>.hstack([[grad_h]]<<A>>)

        (ml_conf.learning_rate)<<A>> = (ml_conf.learning_rate)<<A>> * 0.999
        optim_grad<<A>> = grad<<A>> * ml_conf.learning_rate

        shape_w<<A>> = shape_w<<G>>

        optim_grad_g<<A>> = optim_grad<<A>>.split(shape_w<<A>>[0])[0]
        optim_grad_h<<A>> = optim_grad<<A>>.split(shape_w<<A>>[0])[1]

        optim_grad_G<<G>> = optim_grad_g<<A>>
        optim_grad_H<<H>> = optim_grad_h<<A>>

        W<<G>> = W<<G>> - optim_grad_G<<G>>
        W<<H>> = W<<H>> - optim_grad_H<<H>>

        [[half_ywx]]<<G>> = 0.5 * [[agg_wx]]<<G>> * Y<<G>>

        [[loss]]<<G>> =  ([[half_ywx]]<<G>> * (-1) + [[agg_wx_square]]<<G>> / 8 + np.log(2)).mean()

        loss<<A>> = [[loss]]<<G>>

        if<<A>> pre_loss<<A>> is not None and abs(loss<<A>>.store - pre_loss<<A>>.store) < ml_conf.eps :
            is_stopped = True
        if<<A>> pre_loss<<A>> is None or abs(loss<<A>>.store - pre_loss<<A>>.store) >= ml_conf.eps :
            is_stopped<<A>> = False
        pre_loss<<A>> = loss<<A>>
        if<<A>> is_stopped<<A>>:
            break
        is_stopped<<G>> = is_stopped<<A>>
        if<<G>> is_stopped<<G>>:
            break
        is_stopped<<H>> = is_stopped<<A>>
        if<<H>> is_stopped<<H>>:
            break
        if<<A>> iter_idx >= 2:
            break
        if<<G>> iter_idx >= 2:
            break
        if<<H>> iter_idx >= 2:
            break

Z<<H>> = X<<H>> @ W<<H>>
Z_h<<G>> = Z<<H>>
Z<<G>> = X<<G>> @ W<<G>>
Z_agg<<G>> = Z<<G>> + Z_h<<G>>

Y_test<<G>> = np.array(list(Y<<G>>.store.collect()))[:,1]
Y_pred<<G>> = 1.0 / (1 + (Z_agg<<G>> * -1).map(np.exp))
Y_pred<<G>> = np.array(list(Y_pred.store.collect()))[:,1]
auc<<G>> = metrics.roc_auc_score(Y_test<<G>>, Y_pred<<G>>)
print<<G>>("auc: {}".format(auc))

